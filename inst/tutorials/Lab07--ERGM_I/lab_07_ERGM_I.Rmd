---
title: "Lab 07 - ERGMs - Part one"
output: 
  learnr::tutorial:
    fig_caption: no
    progressive: true
    allow_skip: true
    # toc: true
    # toc_depth: 2
    theme: readable
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
library(learnr)
library(gradethis)
tutorial_options(exercise.checker = gradethis::grade_learnr)
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction
Welcome to the first of two tutorials about Exponential Random Graph Models (ERGMs).
ERGMs are a statistical tool to assess causality using hypothesis testing 
when the outcome variable is a network. 

They are used to assess weather the structure of a network is random or
whether it is originated by some sort of identifiable relational phenomena.


Let's get started with the art of fitting ERGMs!

## Logit

ERG models conceptualize their outcome variable as 

* absence 
or 
* presence 

of an edge between each pair of node in a given network. This set up makes 
the outcome variable binary and makes them very comparable to logistic 
regression, a class of models that focuses of dummy outcome variables. 

Dummy is as nick name for a binary categorical variable such as 

* yes or no
* black or white
* win or lose
* ...

* edge yes or edge no

In `r` you fit a logit model using the `glm` function that belongs to r
base. `glm` stands for generalized linear model. 

Now we know two crucial things about logit models

* they have a dummy dependent variable
* they are estimating linear effects. 

When we use statistical models we are referring to the understanding 
of the the reason why we observe something, rather than the observation
of the behavior of what we observed. The study of why something happens 
is called causal inference, since it deals with inferring (estimating)
the cause of some phenomenon of interest. 

Logit models as much as ERGMs explain why an outcome variable is the way
it is, due to some concurrent phenomena that affected it. That's the reason 
why we say that an explanatory variable predicts an outcome variable. 

Scientists explain phenomena by testing hypotheses. 

* You measure something relevant for you 
my dog is often barking like crazy 

* You formulate an hypothesis on the reason why he is barking
He might be hungry

* You test your hypothesis 
mmm maybe I forgot to feed him again... 

He might be barking also for other reasons such as that he wants to go 
for a walk (the phenomenon is more complex), but there is a certain probability 
associated to your hypothesis being correct. This hypothesis can
be numerically tested measuring how many days in a month
your dog barks like crazy (outcome variable) and how much food he gets every day 
(explanatory variable).


Let's fit the logit model we discussed in the first ERGM lecture to get 
a better understanding of causality and ERGMs! oh... and please, if you have 
a pet, feed her/him!!

```{r load_SuccessData, include = FALSE}
SuccessData <- SNA4DS:::SuccessData
```

First we take a look at the data

```{r head_success, exercise = TRUE, exercise.setup = "load_SuccessData"}

head(SuccessData)
nrow(SuccessData)

```
We have three variables:

* success: whether our respondents are successful or not (dummy)
* numeracy: how much our respondents are good at math (numeric, continuous)
* anxiety: how much our respondents are anxious (numeric, continuous)

Having these three pieces of information about 50 respondents, 
we can ask three 'why' (causal) questions":

* 1. Does the level of success and numeracy predict the presence of anxious thoughts?
* 2. Does the the level of success and anxiety predict numeracy?
* 3. Does the level of anxiety and the level of numeracy predict success?

Even if the data allow us to fit these three models, it is not necessarily 
a good idea to do it. In fact, questions 1 and 2 sound a little detached 
from reality. Why math knowledge should make someone anxious (1)? Why an 
anxious person should be good at math (2)? 

If you decide to run a model you need to support your question with 
evidence to persuade your audience that your question makes sense. 
You also need to provide an attempted answer to your question, informed
by the literature you read on the topic. That's an hypothesis. 

Now, we move on with question 3 assuming that we spent some time reading on 
the topic. According to the literature, is more likely that people good at math
are successful since knowing math allows you to get good jobs. At the same
time, if people work really hard to be successful very often they end up being 
stressed. In accordance, we can formulate the hypothesis:

* H1 The level of anxiety and the level of numeracy are predictors of success.

H1, obviously expects to find effects and it is the opposite of a null hypothesis
of no effect that we call H0

*H0 The level of anxiety and the level of numeracy are not correlated to success.


Then we move on fitting a bunch of models to test H1. Since our outcome variable 
is a dummy, we use a logit model. 
We add the covariates (independent variables) one by one, nesting a series 
of models. 

First we consider only numeracy as an explanatory variable
```{r m1_success, exercise = TRUE, exercise.setup = "load_SuccessData"}
SuccessModel1 <- stats::glm(formula = success ~ numeracy, 
                            family = binomial(link = logit), 
                            data = SuccessData)
summary(SuccessModel1)
```

To fit a model in `r` you always have to specify a formula. 
The first variable in the formula is always the outcome variable. 
We can check the results using the function `summary` from r base

```{r m2_success, exercise = TRUE, exercise.setup = "load_SuccessData"}
SuccessModel2 <- stats::glm(formula = success ~ numeracy + anxiety, 
                            family = binomial(link = logit), 
                            data = SuccessData)
summary(SuccessModel2)
```

We can add more covariates (expanatory variables) with the `+` sign.

Model 2 checks whether respondents that are good at math are successful 
in parallel of checking weather respondents that are anxious are successful. 

If we want to see weather respondents that are good and math and anxious 
at the same time are successful, we need to use an interaction. We do that
adding a third term that multiplies the other two (We can also omit the first 
two terms since the `glm` function individually considers already the terms 
specified in the interaction, but it is better to watch every step now). 
 

```{r m3_success, exercise = TRUE, exercise.setup = "load_SuccessData"}
SuccessModel3 <- stats::glm(formula = success ~ numeracy + anxiety + numeracy * anxiety , 
                            family = binomial(link = logit), 
                            data = SuccessData)
summary(SuccessModel3)
```

That's the final model we consider this time.


## Reading results 

We successfully run our models, but coding is the easy part in this game. 
The real point is: what do these results mean?

In order to understand our results, it is helpful to print them all in once. 
We can do that using the function `screenreg` from the `texreg` package.

Needless to say, you interpret ERGMs results the same way as logit models. 
Hence, let's learn it with this easy example, so when things get more complicated,
later, you are going to be ready!


```{r logit_Models, include = FALSE}
logitModels <- SNA4DS:::logitModels
```

After saving our three models results into a list, we pass the list to 
`screenreg` from the `texreg`package. This package is there with the only goal 
of printing results for you in several formats, saving you a lot of time. 
`screenreg` prints it directly in the `r` console, or in this tutorial. 
Isn't it handy?

Let's first print the results using p-values.

```{r printResPV, exercise = TRUE, exercise.setup = "logit_Models"}

# logitModels <- list(SuccessModel1, SuccessModel2, SuccessModel3)
texreg::screenreg(logitModels)
```

First of all note that having our results next to each other helps 
in getting an overview of our nested models. Model comparison is really 
important, since our goal it to understand which combination of explanatory 
variables predicts our outcome variable more accurately. 

Let's check goodness of fit first. We have three indicators:

* AIC. Akaike information criterion
* BIC. Bayesian information criterion
* Log Likelihood

For AIC and BIC, smaller the value better is the model. 
For Log Likelihood, higer the value better is the model.

In our success case, Model one is definitely the worse among the three. 
According to both AIC and BIC model two is the best one. According to the 
log likelihood, Model two and three are equally better than model one.

In frequentist statistis we can talk about results significance to underline the fact that 
there is a large probability that your result is not random but is the product
of a certain specific phenomena that we observed.

### P-values

Let's take a look at the results considering p-values. In null hypothesis 
significance testing, the p-value is the probability of obtaining test results 
at least as extreme as the results actually observed, under the assumption 
that the null hypothesis is correct.

This definition is quite hard to get since rather that thinking on the effects 
you need to focus on the absence of effects and this twists your brain. 
In practice it means that you want to check on the probability that repeating 
your observation for a large number of times, let's say 1000, your results are
special, not at the center of a normal distribution. If your results are not 
special it means that your hypothesis is wrong since results look the same as if 
the hypothesis was null.  

Stars are a convention to present results:

* coef *** means that the probability that the null hypothesis is true is < 0.001
That's what you want.
* coef ** means that the probability that the null hypothesis is true is < 0.01
* coef * means that the probability that the null hypothesis is true is < 0.05

Higher than that, it's considered more likely that the null hypothesis is true 
and that you found nothing. 

In our study, model three does not look any different than H0. 

In model two numeracy has a 95 % of being different from H0 (significant), 
while anxiety has a probability of 99 % of being different from H0 
(significant). In model one, numeracy has a probability of 99.9 % of being 
different from H0 (significant).  

* p-value < 0.05 -- 5% prob of being the same as H0 / 95% prob of H1 to be correct
* p-value < 0.01 -- 1% prob of being the same as H0 / 99% prob of H1 to be correct
* p-value < 0.001 -- 0.1% prob of being the same as H0 / 99.9% prob of H1 to be correct


Often p-values are used to comment on the extent to which the explanatory 
variable influences the outcome one. P-value tells us nothing about the 
intensity of an effect. It only informs on the probability that your effect could 
be replicated when you repeat your study. 

### Confidence Intervals

Let's explore confidence intervals instead of p-values.
`screenreg` also allows us to print confidence intervals
in place of p-values, by specifying the argument `ci.force = TRUE`.

```{r printResCI, exercise = TRUE, exercise.setup = "logit_Models"}

texreg::screenreg(logitModels, ci.force = TRUE)

```


Using confidence intervals we consider that an upper and a lower bound. If these
two values are both positive or both negative there is a high probability that 
H1 results look different than H0 results, hence the model is significant.

Zero is the null value of the parameter. The upper and the lower bound represent
a 95% confidence interval in reference to a normal distribution.
If the confidence interval includes the null value (0), then there is no 
statistically meaningful or statistically significant difference between the 
observed network and the networks it is compared to (aka it looks like a random one).

The probability of the observed network to be non-random 
(due to some theory driven reasons) are meaningful if the upper and lower 
bound have the same sign.

While the p-value suggest that significance can be more or less intense, the 
confidence interval provides a more careful suggestion on that. Since the models
researchers can run are limited by many possible bias and imperfection. The 
confidence interval provides a more prudent way of understanding results, that 
helps to avoid false positives. 

Confidence intervals can also be plotted with the function`plotreg`. 
A visual understanding of results is always a great help in explaining a
research's output.

```{r plotRes, exercise = TRUE, exercise.setup = "logit_Models"}
texreg::plotreg(logitModels)
```
`plotreg` makes the interpretation of results even easier by showing significant 
results in red with the coefficient represented by a circle, and non-significant 
results in blue with the coefficient represented by a square. 



### Interpreting Coefficients

We can interpret logit models and ERGMs results with odd ratios and
probabilities.

we can calculate 

* odd ratios (OR) by exponentiation the coefficient. 
* probabilities (P) with exp(coef)/(1 + exp(coef)) 
 

```{r plotRes, exercise = TRUE, exercise.setup = "logit_Models"}

# we access the coefficients from the summary of each model 
# from the list, taking the first column of the table 
# of stored results  

coefM2 <- summary(logitModels[[2]])$coef[ , 1]

# we exponentiate each of them with a for loop to compute the OR
or <- NULL

for (i in 1:length(coefM2)) {
 
  temp <- exp(coefM2[i])
  
  or <- append(or, temp)
}

or

# Analogously, we compute the probability

P <- NULL

for (i in 1:length(coefM2)) {
 
  temp <- exp(coefM2[i])/ (1 + exp(coefM2[i]))
  
  P <- append(P, temp)
}

P

```



## Erdos Renyi

## P1

## Dyadic Independent Effects
erdos renyi and P1 are both models with independent effects. But there 
are many more options